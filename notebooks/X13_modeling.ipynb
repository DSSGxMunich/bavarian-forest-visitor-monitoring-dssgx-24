{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pycaret.time_series import *\n",
    "import matplotlib.pyplot as plt\n",
    "import awswrangler as wr\n",
    "import boto3\n",
    "\n",
    "boto3.setup_default_session(profile_name='') # Update with your profile name\n",
    "\n",
    "bucket = \"dssgx-munich-2024-bavarian-forest\"\n",
    "raw_data_folder = \"raw-data\"\n",
    "preprocessed_data_folder = \"preprocessed_data\"\n",
    "\n",
    "def load_csv_files_from_aws_s3(path: str, **kwargs) -> pd.DataFrame:\n",
    "    \"\"\"Loads individual or multiple CSV files from an AWS S3 bucket.\n",
    "    Args:\n",
    "        path (str): The path to the CSV files on AWS S3.\n",
    "        **kwargs: Additional arguments to pass to the read_csv function.\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame containing the data from the CSV files.\n",
    "    \"\"\"\n",
    "    df = wr.s3.read_csv(path=path, **kwargs)\n",
    "    return df\n",
    "df = load_csv_files_from_aws_s3(\n",
    "    path=\"s3://dssgx-munich-2024-bavarian-forest/preprocessed_data/joined_sensor_weather_visitorcenter_2016-2024.csv\"\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Draft\n",
    "\n",
    "- sensor on wich gfäl started working\n",
    "- Target =  Traffic Abs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "df['Time']=pd.to_datetime(df['Time']) \n",
    "\n",
    "df = df.set_index('Time').sort_index()\n",
    "\n",
    "complete_df = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_metric_cols = ['traffic_abs', 'traffic_norm', 'occupancy_abs', 'occupancy_norm', 'sum_IN_norm', 'sum_IN_abs', 'sum_OUT_norm', 'sum_OUT_abs']\n",
    "\n",
    "df.drop(columns = drop_metric_cols, inplace=True)\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_cols = [col for col in df.columns if \"IN\" not in col and \"OUT\" not in col]\n",
    "weather_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of sensors working: \", df.loc[\"2021-10-01 10:00:00\", \"working_sensors\"])\n",
    "df = df[df.index >= \"2021-10-01 10:00:00\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the row corresponding to the given timestamp\n",
    "row = df.loc[\"2021-10-01 10:00:00\"]\n",
    "\n",
    "# Get columns with non-null values\n",
    "non_null_columns = row[row.notnull()].index.tolist()\n",
    "\n",
    "sensor_cols = [col for col in df.columns if \"IN\" in col or \"OUT\" in col]\n",
    "\n",
    "\n",
    "\n",
    "# Display the columns\n",
    "selected_sensors_cols = [col for col in sensor_cols if col in non_null_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_16_sensors = df[selected_sensors_cols + weather_cols]\n",
    "df_16_sensors.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create traffic for 16 sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_16_sensors[\"traffic_abs\"] = df_16_sensors[selected_sensors_cols].sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparition of normalized values from complete_df and 16_sensor_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Assuming `complete_df` and `df` have timestamps as their index\n",
    "# Filter both DataFrames from the given timestamp\n",
    "start_date = \"2021-10-02 10:00:00\"\n",
    "end_date = \"2024-03-31 10:00:00\"\n",
    "complete_df_filtered = complete_df.loc[start_date: end_date]\n",
    "df_filtered = df_16_sensors.loc[start_date: end_date]\n",
    "\n",
    "# Normalize the `traffic_abs` columns using MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Adding traffic_abs to avoid reshaping issues with indexes\n",
    "complete_df_filtered['normalized_traffic_abs'] = scaler.fit_transform(complete_df_filtered[['traffic_abs']])\n",
    "df_filtered['normalized_traffic_abs'] = scaler.fit_transform(df_filtered[['traffic_abs']])\n",
    "\n",
    "# Combine data for plotting\n",
    "combined_df = pd.concat([\n",
    "    complete_df_filtered[['normalized_traffic_abs']].assign(Source='Complete'),\n",
    "    df_filtered[['normalized_traffic_abs']].assign(Source='DF')\n",
    "])\n",
    "\n",
    "# Reset index for plotly compatibility\n",
    "#combined_df.reset_index(inplace=True)\n",
    "\n",
    "# Plot using Plotly Express\n",
    "fig = px.line(\n",
    "    combined_df,\n",
    "    x=combined_df.index,\n",
    "    y='normalized_traffic_abs',\n",
    "    color='Source',\n",
    "    title='Normalized Traffic Over Time',\n",
    "    labels={'index': 'Timestamp', 'normalized_traffic_abs': 'Normalized Traffic'}\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the columns to use\n",
    "columns_to_use = [\n",
    "    'traffic_abs',\n",
    "    'Temperature (°C)',\n",
    "    'Relative Humidity (%)',\n",
    "    'Precipitation (mm)',\n",
    "    'Wind Speed (km/h)',\n",
    "    'Sunshine Duration (min)',\n",
    "    'Monat',\n",
    "    'Wochentag',\n",
    "    'Wochenende',\n",
    "    'Jahreszeit',\n",
    "    'Laubfärbung',\n",
    "    'Feiertag_Bayern',\n",
    "    'Feiertag_CZ',\n",
    "    'HEH_geoeffnet',\n",
    "    'HZW_geoeffnet',\n",
    "    'WGM_geoeffnet',\n",
    "    'Lusenschutzhaus_geoeffnet',\n",
    "    'Racheldiensthuette_geoeffnet',\n",
    "    'Falkensteinschutzhaus_geoeffnet',\n",
    "    'Schwellhaeusl_geoeffnet',\n",
    "    'Schulferien_Bayern',\n",
    "    'Schulferien_CZ',\n",
    "    'Jahr'\n",
    "]\n",
    "\n",
    "df_16_sensors = df_16_sensors[columns_to_use]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype_dict = {\n",
    "    'traffic_abs': 'float64',\n",
    "    'Temperature (°C)': 'float64',\n",
    "    'Relative Humidity (%)': 'float64',\n",
    "    'Precipitation (mm)': 'float64',\n",
    "    'Wind Speed (km/h)': 'float64',\n",
    "    'Sunshine Duration (min)': 'float64',\n",
    "    'Monat': 'float64',\n",
    "    'Wochentag': 'category',\n",
    "    'Wochenende': 'category',\n",
    "    'Jahreszeit': 'category',\n",
    "    'Laubfärbung': 'category',\n",
    "    'Feiertag_Bayern': 'category',\n",
    "    'Feiertag_CZ': 'category',\n",
    "    'HEH_geoeffnet': 'category',\n",
    "    'HZW_geoeffnet': 'category',\n",
    "    'WGM_geoeffnet': 'category',\n",
    "    'Lusenschutzhaus_geoeffnet': 'category',\n",
    "    'Racheldiensthuette_geoeffnet': 'category',\n",
    "    'Falkensteinschutzhaus_geoeffnet': 'category',\n",
    "    'Schwellhaeusl_geoeffnet': 'category',\n",
    "    'Schulferien_Bayern': 'category',\n",
    "    'Schulferien_CZ': 'category',\n",
    "    'Jahr': 'float64'\n",
    "}\n",
    "\n",
    "# Apply the data types to the dataframe\n",
    "df_16_sensors = df_16_sensors.astype(dtype_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_16_sensors = df_16_sensors.loc[:'2024-08-19']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df_16_sensors.asfreq('H')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_cleaned[\"Hour\"] = df_cleaned.index.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the time series module from PyCaret\n",
    "from pycaret.time_series import setup, compare_models, save_model\n",
    "\n",
    "from pycaret.regression import *\n",
    "\n",
    "# Define the target variables\n",
    "targets = ['traffic_abs']\n",
    "\n",
    "numeric_features = ['Temperature (°C)',\n",
    "       'Relative Humidity (%)', 'Precipitation (mm)', 'Wind Speed (km/h)',\n",
    "       'Sunshine Duration (min)']\n",
    "catgorical_features =['Hour','Monat', 'Wochentag', 'Wochenende',\n",
    "       'Jahreszeit', 'Laubfärbung', 'Feiertag_Bayern', 'Feiertag_CZ',\n",
    "       'HEH_geoeffnet', 'HZW_geoeffnet', 'WGM_geoeffnet',\n",
    "       'Lusenschutzhaus_geoeffnet', 'Racheldiensthuette_geoeffnet',\n",
    "       'Falkensteinschutzhaus_geoeffnet', 'Schwellhaeusl_geoeffnet',\n",
    "       'Schulferien_Bayern', 'Schulferien_CZ', 'Jahr']\n",
    "\n",
    "for catfeature in catgorical_features:\n",
    "    df_cleaned[catfeature] = df_cleaned[catfeature].astype(str)\n",
    "\n",
    "\n",
    "\n",
    "# Loop through each target\n",
    "for target in targets:\n",
    "    print(f\"\\nModeling for target: {target}\\n\")\n",
    "\n",
    "    cols_for_modeling = [target] + numeric_features + catgorical_features\n",
    "    \n",
    "    # Initialize the PyCaret setup\n",
    "    ts_setup = setup(\n",
    "        data=df_cleaned[cols_for_modeling],\n",
    "        target=target,\n",
    "        train_size=0.9,\n",
    "        session_id=42,  # For reproducibility\n",
    "       # seasonal_period=24,  # Assumes daily seasonality for hourly data\n",
    "       # fold_strategy='timeseries',  # Use time series cross-validation\n",
    "        data_split_shuffle=True,\n",
    "        fold=3,  # Number of folds in time series cross-validation\n",
    "        #fh=24 * 14,  # Forecast horizon of 2 weeks (24 hours * 14 days)\n",
    "        numeric_features=numeric_features,\n",
    "        categorical_features=catgorical_features,\n",
    "        verbose=False  # Suppress output for clarity\n",
    "    )\n",
    "    \n",
    "    # Compare models and select the best one\n",
    "    best_model = compare_models()\n",
    "    \n",
    "    # Save the best model\n",
    "    save_model(best_model, f'best_model_{target}')\n",
    "\n",
    "    print(f\"Best model for {target} saved.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_model(model, plot = 'feature_all')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_holdout = predict_model(model)\n",
    "pred_holdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "predictions_vs_real = pred_holdout[[\"traffic_abs\", \"prediction_label\"]].sort_index(ascending=True)\n",
    "px.line(predictions_vs_real)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bf_dssdgx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
