{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pycaret.time_series import *\n",
    "from pycaret.regression import *\n",
    "import matplotlib.pyplot as plt\n",
    "import awswrangler as wr\n",
    "import boto3\n",
    "from pycaret import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from pycaret.regression import load_model, plot_model\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "boto3.setup_default_session(profile_name='anthony_garove_fellow_dssgx_24')\n",
    "\n",
    "bucket = \"dssgx-munich-2024-bavarian-forest\"\n",
    "raw_data_folder = \"raw-data\"\n",
    "preprocessed_data_folder = \"preprocessed_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_files_from_aws_s3(path: str, **kwargs) -> pd.DataFrame:\n",
    "    \"\"\"Loads individual or multiple CSV files from an AWS S3 bucket.\n",
    "    Args:\n",
    "        path (str): The path to the CSV files on AWS S3.\n",
    "        **kwargs: Additional arguments to pass to the read_csv function.\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame containing the data from the CSV files.\n",
    "    \"\"\"\n",
    "    df = wr.s3.read_csv(path=path, **kwargs)\n",
    "    return df\n",
    "df = load_csv_files_from_aws_s3(\n",
    "    path=\"s3://dssgx-munich-2024-bavarian-forest/preprocessed_data/joined_sensor_weather_visitorcenter_2016-2024.csv\"\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the columns to use\n",
    "columns_to_use = [\n",
    "'Time',  'Bayerisch Eisenstein IN',  'Bayerisch Eisenstein OUT',  'Brechhäuslau IN',  'Brechhäuslau OUT',  \n",
    "'Deffernik IN',  'Deffernik OUT',  'Diensthüttenstraße IN',  'Diensthüttenstraße OUT',  'Felswandergebiet IN',  \n",
    "'Felswandergebiet OUT',  'Ferdinandsthal IN',  'Ferdinandsthal OUT',  'Fredenbrücke IN',  'Fredenbrücke OUT',  \n",
    "'Gfäll IN',  'Gfäll OUT',  'Gsenget IN',  'Gsenget OUT',  'Klingenbrunner Wald IN',  'Klingenbrunner Wald OUT',  \n",
    "'Klosterfilz IN',  'Klosterfilz OUT',  'Racheldiensthütte IN',  'Racheldiensthütte OUT',  'Sagwassersäge IN',  \n",
    "'Sagwassersäge OUT',  'Scheuereck IN',  'Scheuereck OUT',  'Schillerstraße IN',  'Schillerstraße OUT',  \n",
    "'Schwarzbachbrücke IN',  'Schwarzbachbrücke OUT',  'Falkenstein 2 OUT',  'Falkenstein 2 IN',  'Lusen 2 IN',  \n",
    "'Lusen 2 OUT',  'Lusen 3 IN',  'Lusen 3 OUT',  'Waldhausreibe IN',  'Waldhausreibe OUT',  'Waldspielgelände IN',  \n",
    "'Waldspielgelände OUT',  'Wistlberg IN',  'Wistlberg OUT',  'Bucina MERGED IN',  'Bucina MERGED OUT',  \n",
    "'Falkenstein 1 MERGED IN',  'Falkenstein 1 MERGED OUT',  'Lusen 1 MERGED IN',  'Lusen 1 MERGED OUT',  \n",
    "'Trinkwassertalsperre MERGED IN',  'Trinkwassertalsperre MERGED OUT',  \n",
    "'traffic_abs',  'sum_IN_abs',  'sum_OUT_abs',  'Temperature (°C)',  'Relative Humidity (%)',  \n",
    "'Precipitation (mm)',  'Wind Speed (km/h)',  'Sunshine Duration (min)',  'Tag',  'Monat',  \n",
    "'Wochentag',  'Wochenende',  'Jahreszeit',  'Laubfärbung',  'Schulferien_Bayern',  'Schulferien_CZ',  \n",
    "'Feiertag_Bayern',  'Feiertag_CZ',  'HEH_geoeffnet',  'HZW_geoeffnet',  'WGM_geoeffnet',  \n",
    "'Lusenschutzhaus_geoeffnet',  'Racheldiensthuette_geoeffnet',  'Falkensteinschutzhaus_geoeffnet',  \n",
    "'Schwellhaeusl_geoeffnet'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the dataframe to only include the specified columns\n",
    "df = df[columns_to_use]\n",
    "\n",
    "# Display the first few rows to ensure the data is loaded correctly\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE NEW REGION VARIABLE\n",
    "\n",
    "# Remove MERGED from column names with this unnecessary label\n",
    "df.columns = df.columns.str.replace(' MERGED', '', regex=False)\n",
    "\n",
    "# Create a dictionary for mapping\n",
    "location_mapping = {\n",
    "    'Bayerisch Eisenstein IN': 'Falkenstein-Schwellhäusl',\n",
    "    'Bayerisch Eisenstein OUT': 'Falkenstein-Schwellhäusl', \n",
    "    'Brechhäuslau IN': 'Falkenstein-Schwellhäusl', \n",
    "    'Brechhäuslau OUT': 'Falkenstein-Schwellhäusl', \n",
    "    'Deffernik IN': 'Falkenstein-Schwellhäusl',\n",
    "    'Deffernik OUT': 'Falkenstein-Schwellhäusl',\n",
    "    'Falkenstein 1 IN': 'Nationalparkzentrum Falkenstein', \n",
    "    'Falkenstein 1 OUT': 'Nationalparkzentrum Falkenstein',\n",
    "    'Falkenstein 2 IN': 'Nationalparkzentrum Falkenstein', \n",
    "    'Falkenstein 2 OUT': 'Nationalparkzentrum Falkenstein',\n",
    "    'Ferdinandsthal IN': 'Falkenstein-Schwellhäusl', \n",
    "    'Ferdinandsthal OUT': 'Falkenstein-Schwellhäusl', \n",
    "    'Gsenget IN': 'Scheuereck-Schachten-Trinkwassertalsperre', \n",
    "    'Gsenget OUT': 'Scheuereck-Schachten-Trinkwassertalsperre', \n",
    "    'Scheuereck IN': 'Scheuereck-Schachten-Trinkwassertalsperre',\n",
    "    'Scheuereck OUT': 'Scheuereck-Schachten-Trinkwassertalsperre', \n",
    "    'Schillerstraße IN': 'Falkenstein-Schwellhäusl', \n",
    "    'Schillerstraße OUT': 'Falkenstein-Schwellhäusl', \n",
    "    'Trinkwassertalsperre IN': 'Scheuereck-Schachten-Trinkwassertalsperre',\n",
    "    'Trinkwassertalsperre OUT': 'Scheuereck-Schachten-Trinkwassertalsperre',\n",
    "    'Bucina IN': 'Lusen-Mauth-Finsterau',\n",
    "    'Bucina OUT': 'Lusen-Mauth-Finsterau', \n",
    "    'Diensthüttenstraße IN': 'Rachel-Spiegelau', \n",
    "    'Diensthüttenstraße OUT': 'Rachel-Spiegelau',\n",
    "    'Felswandergebiet IN': 'Lusen-Mauth-Finsterau', \n",
    "    'Felswandergebiet OUT': 'Lusen-Mauth-Finsterau',\n",
    "    'Fredenbrücke IN': 'Lusen-Mauth-Finsterau', \n",
    "    'Fredenbrücke OUT': 'Lusen-Mauth-Finsterau', \n",
    "    'Gfäll IN': 'Rachel-Spiegelau', \n",
    "    'Gfäll OUT': 'Rachel-Spiegelau', \n",
    "    'Klingenbrunner Wald IN': 'Rachel-Spiegelau', \n",
    "    'Klingenbrunner Wald OUT': 'Rachel-Spiegelau', \n",
    "    'Klosterfilz IN': 'Rachel-Spiegelau', \n",
    "    'Klosterfilz OUT': 'Rachel-Spiegelau',\n",
    "    'Lusen 1 IN': 'Nationalparkzentrum Lusen', \n",
    "    'Lusen 1 OUT': 'Nationalparkzentrum Lusen', \n",
    "    'Lusen 2 IN': 'Nationalparkzentrum Lusen',\n",
    "    'Lusen 2 OUT': 'Nationalparkzentrum Lusen', \n",
    "    'Lusen 3 IN': 'Nationalparkzentrum Lusen', \n",
    "    'Lusen 3 OUT': 'Nationalparkzentrum Lusen',\n",
    "    'Racheldiensthütte IN': 'Rachel-Spiegelau', \n",
    "    'Racheldiensthütte OUT': 'Rachel-Spiegelau',\n",
    "    'Schwarzbachbrücke IN': 'Lusen-Mauth-Finsterau', \n",
    "    'Schwarzbachbrücke OUT': 'Lusen-Mauth-Finsterau', \n",
    "    'Waldhausreibe IN': 'Lusen-Mauth-Finsterau', \n",
    "    'Waldhausreibe OUT': 'Lusen-Mauth-Finsterau', \n",
    "    'Waldspielgelände IN': 'Rachel-Spiegelau', \n",
    "    'Waldspielgelände OUT': 'Rachel-Spiegelau', \n",
    "    'Wistlberg IN': 'Lusen-Mauth-Finsterau', \n",
    "    'Wistlberg OUT': 'Lusen-Mauth-Finsterau', \n",
    "    'Sagwassersäge IN': 'Lusen-Mauth-Finsterau',\n",
    "    'Sagwassersäge OUT': 'Lusen-Mauth-Finsterau'\n",
    "}\n",
    "\n",
    "# Extract unique regions\n",
    "regions = set(location_mapping.values())\n",
    "\n",
    "# Iterate over each region\n",
    "for region in regions:\n",
    "    # Filter the keys in location_mapping that belong to the current region\n",
    "    region_in_columns = [col for col in location_mapping if location_mapping[col] == region and ' IN' in col]\n",
    "    region_out_columns = [col for col in location_mapping if location_mapping[col] == region and ' OUT' in col]\n",
    "\n",
    "    # Sum the values for all IN columns of the current region, while retaining NaN where all are NaN\n",
    "    df[f'{region} IN'] = df[region_in_columns].sum(axis=1, min_count=1)\n",
    "    \n",
    "    # Sum the values for all OUT columns of the current region, while retaining NaN where all are NaN\n",
    "    df[f'{region} OUT'] = df[region_out_columns].sum(axis=1, min_count=1)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "df.tail()\n",
    "\n",
    "# EXPLANATION OF LOOPING FUNCTION BELOW\n",
    "#min_count=1 in sum():\n",
    "#The sum(axis=1, min_count=1) method ensures that if all values being summed are NaN, the result will be NaN.\n",
    "#If at least one value is not NaN, it will compute the sum, ignoring the NaN values.\n",
    "#Explanation:\n",
    "#min_count=1: This parameter in the sum() function specifies the minimum number of non-NaN values required to perform the summation. If the count of non-NaN values is less than min_count, the result will be NaN.\n",
    "#Result: The DataFrame will have the new region columns that sum the sensors while retaining NaN if all sensors in a region are NaN for a given row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_files_from_aws_s3(path: str, **kwargs) -> pd.DataFrame:\n",
    "    \"\"\"Loads individual or multiple CSV files from an AWS S3 bucket.\n",
    "    Args:\n",
    "        path (str): The path to the CSV files on AWS S3.\n",
    "        **kwargs: Additional arguments to pass to the read_csv function.\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame containing the data from the CSV files.\n",
    "    \"\"\"\n",
    "    df = wr.s3.read_csv(path=path, **kwargs)\n",
    "    return df\n",
    "df_newfeatures = load_csv_files_from_aws_s3(\n",
    "    path=\"s3://dssgx-munich-2024-bavarian-forest/preprocessed_data/holidays_deltaweather_features_df.csv\"\n",
    ")\n",
    "df_newfeatures.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Time'] = pd.to_datetime(df['Time'])\n",
    "df_newfeatures['Time'] = pd.to_datetime(df_newfeatures['Time'])\n",
    "\n",
    "# Step 2: Select the columns you want to add from df_newfeatures\n",
    "columns_to_add = [\n",
    "    'ZScore_Daily_Max_Temperature (°C)',\n",
    "    'ZScore_Daily_Max_Relative Humidity (%)',\n",
    "    'ZScore_Daily_Max_Precipitation (mm)',\n",
    "    'ZScore_Daily_Max_Wind Speed (km/h)',\n",
    "    'ZScore_Daily_Max_Sunshine Duration (min)',\n",
    "    'Distance_to_Nearest_Holiday_Bayern',\n",
    "    'Distance_to_Nearest_Holiday_CZ'\n",
    "]\n",
    "\n",
    "# Ensure that the selected columns exist in df_newfeatures\n",
    "selected_columns = [col for col in columns_to_add if col in df_newfeatures.columns]\n",
    "\n",
    "# Step 3: Merge df with df_newfeatures on 'Time' and add the selected columns\n",
    "df = pd.merge(df, df_newfeatures[['Time'] + selected_columns], on='Time', how='left')\n",
    "\n",
    "# Optionally, you can display the merged dataframe\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the data types\n",
    "dtype_dict = {\n",
    "    'Time': 'datetime64[ns]',\n",
    "    'traffic_abs': 'float64',\n",
    "    'Temperature (°C)': 'float64',\n",
    "    'Relative Humidity (%)': 'float64',\n",
    "    'Precipitation (mm)': 'float64',\n",
    "    'Wind Speed (km/h)': 'float64',\n",
    "    'Sunshine Duration (min)': 'float64',\n",
    "    'Monat': 'float64',\n",
    "    'Wochentag': 'category',\n",
    "    'Wochenende': 'category',\n",
    "    'Jahreszeit': 'category',\n",
    "    'Laubfärbung': 'category',\n",
    "    'Feiertag_Bayern': 'category',\n",
    "    'Feiertag_CZ': 'category',\n",
    "    'HEH_geoeffnet': 'category',\n",
    "    'HZW_geoeffnet': 'category',\n",
    "    'WGM_geoeffnet': 'category',\n",
    "    'Lusenschutzhaus_geoeffnet': 'category',\n",
    "    'Racheldiensthuette_geoeffnet': 'category',\n",
    "    'Falkensteinschutzhaus_geoeffnet': 'category',\n",
    "    'Schwellhaeusl_geoeffnet': 'category',\n",
    "    'Schulferien_Bayern': 'category',\n",
    "    'Schulferien_CZ': 'category',\n",
    "    'sum_IN_abs': 'float64',\n",
    "    'sum_OUT_abs': 'float64',\n",
    "    'Falkenstein-Schwellhäusl IN': 'float64',\n",
    "    'Rachel-Spiegelau IN': 'float64',\n",
    "    'Nationalparkzentrum Falkenstein IN': 'float64',\n",
    "    'Nationalparkzentrum Lusen IN': 'float64',\n",
    "    'Lusen-Mauth-Finsterau IN': 'float64',\n",
    "    'Scheuereck-Schachten-Trinkwassertalsperre IN': 'float64',\n",
    "    'Falkenstein-Schwellhäusl OUT': 'float64',\n",
    "    'Rachel-Spiegelau OUT': 'float64',\n",
    "    'Nationalparkzentrum Falkenstein OUT': 'float64',\n",
    "    'Nationalparkzentrum Lusen OUT': 'float64',\n",
    "    'Lusen-Mauth-Finsterau OUT': 'float64',\n",
    "    'Scheuereck-Schachten-Trinkwassertalsperre OUT': 'float64',\n",
    "    'Bayerisch Eisenstein IN': 'float64',\n",
    "    'Bayerisch Eisenstein OUT': 'float64',\n",
    "    'Brechhäuslau IN': 'float64',\n",
    "    'Brechhäuslau OUT': 'float64',\n",
    "    'Deffernik IN': 'float64',\n",
    "    'Deffernik OUT': 'float64',\n",
    "    'Diensthüttenstraße IN': 'float64',\n",
    "    'Diensthüttenstraße OUT': 'float64',\n",
    "    'Felswandergebiet IN': 'float64',\n",
    "    'Felswandergebiet OUT': 'float64',\n",
    "    'Ferdinandsthal IN': 'float64',\n",
    "    'Ferdinandsthal OUT': 'float64',\n",
    "    'Fredenbrücke IN': 'float64',\n",
    "    'Fredenbrücke OUT': 'float64',\n",
    "    'Gfäll IN': 'float64',\n",
    "    'Gfäll OUT': 'float64',\n",
    "    'Gsenget IN': 'float64',\n",
    "    'Gsenget OUT': 'float64',\n",
    "    'Klingenbrunner Wald IN': 'float64',\n",
    "    'Klingenbrunner Wald OUT': 'float64',\n",
    "    'Klosterfilz IN': 'float64',\n",
    "    'Klosterfilz OUT': 'float64',\n",
    "    'Racheldiensthütte IN': 'float64',\n",
    "    'Racheldiensthütte OUT': 'float64',\n",
    "    'Sagwassersäge IN': 'float64',\n",
    "    'Sagwassersäge OUT': 'float64',\n",
    "    'Scheuereck IN': 'float64',\n",
    "    'Scheuereck OUT': 'float64',\n",
    "    'Schillerstraße IN': 'float64',\n",
    "    'Schillerstraße OUT': 'float64',\n",
    "    'Schwarzbachbrücke IN': 'float64',\n",
    "    'Schwarzbachbrücke OUT': 'float64',\n",
    "    'Falkenstein 2 OUT': 'float64',\n",
    "    'Falkenstein 2 IN': 'float64',\n",
    "    'Lusen 2 IN': 'float64',\n",
    "    'Lusen 2 OUT': 'float64',\n",
    "    'Lusen 3 IN': 'float64',\n",
    "    'Lusen 3 OUT': 'float64',\n",
    "    'Waldhausreibe IN': 'float64',\n",
    "    'Waldhausreibe OUT': 'float64',\n",
    "    'Waldspielgelände IN': 'float64',\n",
    "    'Waldspielgelände OUT': 'float64',\n",
    "    'Wistlberg IN': 'float64',\n",
    "    'Wistlberg OUT': 'float64',\n",
    "    'Bucina IN': 'float64',\n",
    "    'Bucina OUT': 'float64',\n",
    "    'Falkenstein 1 IN': 'float64',\n",
    "    'Falkenstein 1 OUT': 'float64',\n",
    "    'Lusen 1 IN': 'float64',\n",
    "    'Lusen 1 OUT': 'float64',\n",
    "    'Trinkwassertalsperre IN': 'float64',\n",
    "    'Trinkwassertalsperre OUT': 'float64',\n",
    "    'ZScore_Daily_Max_Temperature (°C)': 'float64',\n",
    "    'ZScore_Daily_Max_Relative Humidity (%)': 'float64',\n",
    "    'ZScore_Daily_Max_Precipitation (mm)': 'float64',\n",
    "    'ZScore_Daily_Max_Wind Speed (km/h)': 'float64',\n",
    "    'ZScore_Daily_Max_Sunshine Duration (min)': 'float64',\n",
    "    'Distance_to_Nearest_Holiday_Bayern': 'float64',\n",
    "    'Distance_to_Nearest_Holiday_CZ': 'float64'\n",
    "}\n",
    "\n",
    "# Apply data types\n",
    "df = df.astype(dtype_dict)\n",
    "\n",
    "# Set 'Time' column as index\n",
    "df.set_index('Time', inplace=True)\n",
    "\n",
    "# Add 'Hour' column based on the index\n",
    "df[\"Hour\"] = df.index.hour\n",
    "\n",
    "# Convert 'Hour' to categorical\n",
    "df['Hour'] = pd.Categorical(df['Hour'])\n",
    "\n",
    "# Reset the index to make 'Time' a column again\n",
    "df.reset_index(inplace=True)\n",
    "\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'Time' is in datetime format\n",
    "df['Time'] = pd.to_datetime(df['Time'])\n",
    "\n",
    "# Set 'Time' as the index\n",
    "df.set_index('Time', inplace=True)\n",
    "\n",
    "# Slice the data from January 1, 2023, to August 19, 2024\n",
    "df = df.loc['2023-01-01':'2024-08-19']\n",
    "# Display the info to check data types\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_columns = [\n",
    "    'Falkenstein-Schwellhäusl IN', \n",
    "    'Rachel-Spiegelau IN', \n",
    "    'Nationalparkzentrum Falkenstein IN',\n",
    "    'Nationalparkzentrum Lusen IN', \n",
    "    'Lusen-Mauth-Finsterau IN', \n",
    "    'Scheuereck-Schachten-Trinkwassertalsperre IN',\n",
    "    'Falkenstein-Schwellhäusl OUT', \n",
    "    'Rachel-Spiegelau OUT', \n",
    "    'Nationalparkzentrum Falkenstein OUT',\n",
    "    'Nationalparkzentrum Lusen OUT', \n",
    "    'Lusen-Mauth-Finsterau OUT', \n",
    "    'Scheuereck-Schachten-Trinkwassertalsperre OUT'\n",
    "]\n",
    "\n",
    "for column in region_columns:\n",
    "    if column in df.columns:\n",
    "        missing_dates = df[df[column].isna()].index\n",
    "        if not missing_dates.empty:\n",
    "            first_missing_date = missing_dates[0]\n",
    "            print(f\"Column '{column}' has its first missing value on {first_missing_date}\")\n",
    "        else:\n",
    "            print(f\"Column '{column}' has no missing values\")\n",
    "    else:\n",
    "        print(f\"Column '{column}' is not in the DataFrame\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slice the data from January 1, 2023, to July 22, 2024\n",
    "df = df.loc['2023-01-01':'2024-07-22']\n",
    "# Display the info to check data types\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in the dataset\n",
    "missing_values = df.isnull().sum()\n",
    "print(missing_values[missing_values > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features to apply cyclic transformation\n",
    "cyclic_features = ['Tag', 'Monat', 'Hour', 'Wochentag']\n",
    "\n",
    "# Convert categorical features to numeric if they are not already\n",
    "for feature in cyclic_features:\n",
    "    if feature in df.columns:\n",
    "        if pd.api.types.is_categorical_dtype(df[feature]):\n",
    "            df[feature] = df[feature].cat.codes  # Convert categorical to numeric codes\n",
    "        \n",
    "        max_value = df[feature].max()  # Get max value for scaling\n",
    "        \n",
    "        # Apply sine and cosine transformations\n",
    "        df[f'{feature}_sin'] = np.sin(2 * np.pi * df[feature] / max_value)\n",
    "        df[f'{feature}_cos'] = np.cos(2 * np.pi * df[feature] / max_value)\n",
    "        \n",
    "        # Drop the original feature column\n",
    "        df.drop(columns=[feature], inplace=True)\n",
    "    else:\n",
    "        print(f\"Warning: Feature '{feature}' not found in DataFrame\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of numeric features to normalize\n",
    "standardized_features = ['Temperature (°C)', 'Relative Humidity (%)', 'Precipitation (mm)', 'Wind Speed (km/h)', 'Sunshine Duration (min)']\n",
    "\n",
    "# Loop through each numeric feature and apply z-score normalization\n",
    "for feature in standardized_features:\n",
    "    if feature in df.columns:\n",
    "        mean_value = df[feature].mean()  # Calculate mean\n",
    "        std_value = df[feature].std()    # Calculate standard deviation\n",
    "        \n",
    "        # Apply z-score normalization\n",
    "        df[feature] = (df[feature] - mean_value) / std_value\n",
    "    else:\n",
    "        print(f\"Warning: Feature '{feature}' not found in DataFrame\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot-encode season variable\n",
    "# Apply one-hot encoding to 'Jahreszeit' and update the original DataFrame\n",
    "df = pd.get_dummies(df, columns=['Jahreszeit'], prefix='Jahreszeit')\n",
    "\n",
    "# Print the names of the new columns\n",
    "new_columns = df.columns\n",
    "print(\"New columns after one-hot encoding 'Jahreszeit':\")\n",
    "print(new_columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of categorical columns to update\n",
    "categorical_columns = [\n",
    "    'Wochenende',\n",
    "    'Jahreszeit_Frühling',\n",
    "    'Jahreszeit_Herbst',\n",
    "    'Jahreszeit_Sommer',\n",
    "    'Jahreszeit_Winter',\n",
    "    'Laubfärbung',\n",
    "    'Schulferien_Bayern',\n",
    "    'Schulferien_CZ',\n",
    "    'Feiertag_Bayern',\n",
    "    'Feiertag_CZ',\n",
    "    'HEH_geoeffnet',\n",
    "    'HZW_geoeffnet',\n",
    "    'WGM_geoeffnet',\n",
    "    'Lusenschutzhaus_geoeffnet',\n",
    "    'Racheldiensthuette_geoeffnet',\n",
    "    'Falkensteinschutzhaus_geoeffnet',\n",
    "    'Schwellhaeusl_geoeffnet'\n",
    "]\n",
    "\n",
    "# Convert specified columns to categorical type and replace TRUE/FALSE with 1/0\n",
    "for col in categorical_columns:\n",
    "    if col in df.columns:\n",
    "        # Replace TRUE with 1 and FALSE with 0\n",
    "        df[col] = df[col].replace({True: 1, False: 0})\n",
    "        \n",
    "        # Convert column to categorical type\n",
    "        df[col] = df[col].astype('category')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target and feature columns\n",
    "target_vars_et = ['traffic_abs', 'sum_IN_abs', 'sum_OUT_abs', 'Lusen-Mauth-Finsterau IN', 'Lusen-Mauth-Finsterau OUT', \n",
    "               'Nationalparkzentrum Lusen IN', 'Nationalparkzentrum Lusen OUT', 'Rachel-Spiegelau IN', 'Rachel-Spiegelau OUT', \n",
    "               'Falkenstein-Schwellhäusl IN', 'Falkenstein-Schwellhäusl OUT', \n",
    "               'Scheuereck-Schachten-Trinkwassertalsperre IN', 'Scheuereck-Schachten-Trinkwassertalsperre OUT', \n",
    "               'Nationalparkzentrum Falkenstein IN', 'Nationalparkzentrum Falkenstein OUT']\n",
    "\n",
    "numeric_features = ['Tag_sin', 'Tag_cos', 'Monat_sin', 'Monat_cos', 'Hour_sin', 'Hour_cos', 'Wochentag_sin', 'Wochentag_cos',\n",
    "                    'Temperature (°C)', 'Relative Humidity (%)', 'Precipitation (mm)', 'Wind Speed (km/h)', \n",
    "                    'Sunshine Duration (min)', 'ZScore_Daily_Max_Temperature (°C)', \n",
    "                    'ZScore_Daily_Max_Relative Humidity (%)','ZScore_Daily_Max_Precipitation (mm)',\n",
    "                    'ZScore_Daily_Max_Wind Speed (km/h)','ZScore_Daily_Max_Sunshine Duration (min)',\n",
    "                    'Distance_to_Nearest_Holiday_Bayern','Distance_to_Nearest_Holiday_CZ']\n",
    "\n",
    "categorical_features = ['Wochenende', 'Jahreszeit_Frühling', 'Jahreszeit_Herbst', 'Jahreszeit_Sommer', 'Jahreszeit_Winter', \n",
    "                        'Laubfärbung', 'Schulferien_Bayern', 'Schulferien_CZ', \n",
    "                        'Feiertag_Bayern', 'Feiertag_CZ', 'HEH_geoeffnet', 'HZW_geoeffnet', 'WGM_geoeffnet', \n",
    "                        'Lusenschutzhaus_geoeffnet', 'Racheldiensthuette_geoeffnet', 'Falkensteinschutzhaus_geoeffnet', \n",
    "                        'Schwellhaeusl_geoeffnet']\n",
    "\n",
    "for catfeature in categorical_features: \n",
    "    df[catfeature] = df[catfeature].astype(str)\n",
    "\n",
    "# Dictionary to store dataframes\n",
    "target_dataframes_et = {}\n",
    "\n",
    "# Iterate over each target variable\n",
    "for target in target_vars_et:\n",
    "    if target in df.columns:\n",
    "        # Select the target variable and features\n",
    "        target_df_et = df[numeric_features + categorical_features + [target]].copy()\n",
    "        target_dataframes_et[target] = target_df_et\n",
    "        print(f\"DataFrame for target variable '{target}' created.\")\n",
    "    else:\n",
    "        print(f\"Target variable '{target}' is not in the DataFrame columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_dict_et = {}\n",
    "\n",
    "save_path = r\"C:\\Users\\garov\\OneDrive\\Documents\\GitHub\\bavarian-forest-visitor-monitoring-dssgx-24\"\n",
    "\n",
    "for target in target_vars_et:\n",
    "    # Ensure the target is in the dictionary of processed DataFrames\n",
    "    if target in target_dataframes_et:\n",
    "        df = target_dataframes_et[target]\n",
    "        \n",
    "        # Ensure the DataFrame has a date-time index\n",
    "        if isinstance(df.index, pd.DatetimeIndex):\n",
    "            # Define date ranges for training, testing, and unseen data\n",
    "            train_start = '2023-01-01'\n",
    "            train_end = '2023-12-31'\n",
    "            test_start = '2024-01-01'\n",
    "            test_end = '2024-04-30'\n",
    "            unseen_start = '2024-05-01'\n",
    "            unseen_end = '2024-07-22'\n",
    "            \n",
    "            # Split the data into train, test, and unseen sets based on date ranges\n",
    "            df_train = df.loc[train_start:train_end]\n",
    "            df_test = df.loc[test_start:test_end]\n",
    "            df_unseen = df.loc[unseen_start:unseen_end]\n",
    "            \n",
    "            # Combine train and test data for model training\n",
    "            df_train_and_test = pd.concat([df_train, df_test])\n",
    "            \n",
    "            # Setup PyCaret for the target variable with the combined data\n",
    "            reg_setup = setup(data=df_train_and_test,\n",
    "                              target=target, \n",
    "                              numeric_features=numeric_features, \n",
    "                              categorical_features=categorical_features,\n",
    "                              fold=5,\n",
    "                              preprocess=True,\n",
    "                              data_split_shuffle=False,  # Do not shuffle data to maintain date order\n",
    "                              session_id=123,\n",
    "                              train_size=0.9)  # Use 90% of data for training \n",
    "            \n",
    "            # Train the Extra Trees Regressor model\n",
    "            extra_trees_model = create_model('et')\n",
    "            \n",
    "            # Predict on the unseen data\n",
    "            predictions_unseen = predict_model(extra_trees_model, data=df_unseen)\n",
    "            \n",
    "            # Save the model\n",
    "            save_model(extra_trees_model, f\"{save_path}/extra_trees_{target}\")\n",
    "            \n",
    "            # Save the predictions in the dictionary for future use\n",
    "            predictions_dict_et[f\"extra_trees_{target}\"] = predictions_unseen\n",
    "            \n",
    "            print(f\"Predictions for unseen data saved for {target}\") \n",
    "            \n",
    "            # Optionally, save the predictions to a CSV\n",
    "            #predictions_unseen.to_csv(f\"{save_path}/predictions_unseen_{target}.csv\", index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to load models\n",
    "save_path = r\"C:\\Users\\garov\\OneDrive\\Documents\\GitHub\\bavarian-forest-visitor-monitoring-dssgx-24\"\n",
    "\n",
    "# Define target variable lists\n",
    "target_vars_et = ['traffic_abs', 'sum_IN_abs', 'sum_OUT_abs', 'Lusen-Mauth-Finsterau IN', 'Lusen-Mauth-Finsterau OUT', \n",
    "               'Nationalparkzentrum Lusen IN', 'Nationalparkzentrum Lusen OUT', 'Rachel-Spiegelau IN', 'Rachel-Spiegelau OUT', \n",
    "               'Falkenstein-Schwellhäusl IN', 'Falkenstein-Schwellhäusl OUT', \n",
    "               'Scheuereck-Schachten-Trinkwassertalsperre IN', 'Scheuereck-Schachten-Trinkwassertalsperre OUT', \n",
    "               'Nationalparkzentrum Falkenstein IN', 'Nationalparkzentrum Falkenstein OUT']\n",
    "\n",
    "\n",
    "# Plot feature importance for Extra Trees models\n",
    "for target in target_vars_et:\n",
    "    model_filename = f'extra_trees_{target}'\n",
    "    full_model_path = os.path.join(save_path, model_filename)\n",
    "    \n",
    "    try:\n",
    "        # Load the saved model\n",
    "        loaded_model = load_model(full_model_path)\n",
    "        \n",
    "        # Plot feature importance\n",
    "        print(f\"Feature importance for Extra Trees model on target '{target}':\")\n",
    "        plot_model(loaded_model, plot='feature_all')\n",
    "    \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"File not found: {e}\")\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions for Extra Trees models\n",
    "for key, predictions_et in predictions_dict_et.items():\n",
    "    target = key.split('_', 2)[-1]  # This assumes the format 'extra_trees_<target>'\n",
    "    \n",
    "    if \"prediction_label\" in predictions_et.columns and target in predictions_et.columns:\n",
    "        predictions_vs_real_et = predictions_et[[target, \"prediction_label\"]].sort_index(ascending=True)\n",
    "        \n",
    "        # Create a line plot using Matplotlib\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(predictions_vs_real_et.index, predictions_vs_real_et[target], label='Actual', color='blue')\n",
    "        plt.plot(predictions_vs_real_et.index, predictions_vs_real_et[\"prediction_label\"], label='Predicted', color='red')\n",
    "        plt.xlabel('Index')\n",
    "        plt.ylabel('Value')\n",
    "        plt.title(f\"Predictions vs. Real Values for {key}\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"Columns '{target}' and 'prediction_label' not found in predictions for {key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, predictions_et in predictions_dict_et.items():\n",
    "    target = key.split('_', 2)[-1]  # This assumes the format 'extra_trees_<target>'\n",
    "    \n",
    "    if \"prediction_label\" in predictions_et.columns and target in predictions_et.columns:\n",
    "        # Resample predictions and actual values on a daily basis\n",
    "        daily_prediction_comparison = predictions_et[[target, \"prediction_label\"]].resample(\"1d\").sum()\n",
    "\n",
    "        # Calculate the mean absolute error (MAE)\n",
    "        daily_prediction_comparison[\"mae\"] = abs(daily_prediction_comparison[target] - daily_prediction_comparison[\"prediction_label\"])\n",
    "        print(f\"The MAE on a daily basis for {target} is {daily_prediction_comparison['mae'].mean()}.\")\n",
    "\n",
    "        # Plot the actual vs predicted values using Plotly Express\n",
    "        fig = px.line(daily_prediction_comparison, y=[target, \"prediction_label\"],\n",
    "                      labels={\"value\": \"Value\", \"variable\": \"Legend\"},\n",
    "                      title=f\"Daily Predictions vs Actuals for {target}\")\n",
    "        fig.show()\n",
    "\n",
    "    else:\n",
    "        print(f\"Columns '{target}' and 'prediction_label' not found in predictions for {key}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, predictions_et in predictions_dict_et.items():\n",
    "    target = key.split('_', 2)[-1]  # This assumes the format 'extra_trees_<target>'\n",
    "    \n",
    "    if \"prediction_label\" in predictions_et.columns and target in predictions_et.columns:\n",
    "        # Resample predictions and actual values on a daily basis\n",
    "        daily_prediction_comparison = predictions_et[[target, \"prediction_label\"]].resample(\"1d\").sum()\n",
    "\n",
    "        # Calculate the mean absolute error (MAE) on a daily basis\n",
    "        daily_prediction_comparison[\"mae\"] = abs(daily_prediction_comparison[target] - daily_prediction_comparison[\"prediction_label\"])\n",
    "\n",
    "        # Print the average number of people visiting the park (or any other target)\n",
    "        print(f\"On average, {daily_prediction_comparison[target].mean()} people are visiting the park daily for {target}.\")\n",
    "\n",
    "        # Create a box plot of the MAE using Plotly Express\n",
    "        fig_box = px.box(daily_prediction_comparison, y=\"mae\", title=f\"MAE Distribution for {target}\")\n",
    "        fig_box.show()\n",
    "\n",
    "        # Identify the top 50 days with the highest error\n",
    "        high_error_dates = daily_prediction_comparison[\"mae\"].sort_values(ascending=False).head(50)\n",
    "        print(high_error_dates)\n",
    "\n",
    "        # Retrieve and print training data columns using PyCaret's get_config function\n",
    "        X_train = get_config('X_train')\n",
    "        X_train_columns = X_train.columns.to_list()\n",
    "        print(f\"Training columns for {target}: {X_train_columns}\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Columns '{target}' and 'prediction_label' not found in predictions for {key}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define start and end dates for inference\n",
    "start_date = \"2024-08-30 00:00\"\n",
    "end_date = \"2024-09-07 23:00\"\n",
    "\n",
    "# Create an hourly date range for inference\n",
    "inference_index = pd.date_range(\n",
    "    start=pd.to_datetime(start_date),\n",
    "    end=pd.to_datetime(end_date),\n",
    "    freq=\"1h\"\n",
    ")\n",
    "\n",
    "# Create an empty DataFrame for inference with X_train columns\n",
    "inference_df = pd.DataFrame(index=inference_index, columns=X_train_columns)\n",
    "\n",
    "# Add date-related features using cyclic transformations (e.g., Hour, Monat)\n",
    "inference_df[\"Hour\"] = inference_df.index.hour\n",
    "inference_df[\"Monat\"] = inference_df.index.month\n",
    "inference_df[\"Jahr\"] = inference_df.index.year\n",
    "\n",
    "inference_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_files_from_aws_s3(path: str, **kwargs) -> pd.DataFrame:\n",
    "    \"\"\"Loads individual or multiple CSV files from an AWS S3 bucket.\n",
    "    Args:\n",
    "        path (str): The path to the CSV files on AWS S3.\n",
    "        **kwargs: Additional arguments to pass to the read_csv function.\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame containing the data from the CSV files.\n",
    "    \"\"\"\n",
    "    df2 = wr.s3.read_csv(path=path, **kwargs)\n",
    "    return df2\n",
    "df2 = load_csv_files_from_aws_s3(\n",
    "    path=\"s3://dssgx-munich-2024-bavarian-forest/preprocessed_data/df_visitcenters_hourly.csv\"\n",
    ")\n",
    "\n",
    "df2.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_files_from_aws_s3(path: str, **kwargs) -> pd.DataFrame:\n",
    "    \"\"\"Loads individual or multiple CSV files from an AWS S3 bucket.\n",
    "    Args:\n",
    "        path (str): The path to the CSV files on AWS S3.\n",
    "        **kwargs: Additional arguments to pass to the read_csv function.\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame containing the data from the CSV files.\n",
    "    \"\"\"\n",
    "    df3 = wr.s3.read_csv(path=path, **kwargs)\n",
    "    return df3\n",
    "df3 = load_csv_files_from_aws_s3(\n",
    "    path=\"s3://dssgx-munich-2024-bavarian-forest/preprocessed_data/processed_weather_data_inference_df.csv\"\n",
    ")\n",
    "\n",
    "df3.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the Time column in df2 and df3 to datetime\n",
    "df2['Time'] = pd.to_datetime(df2['Time'])\n",
    "df3['Time'] = pd.to_datetime(df3['Time'])\n",
    "\n",
    "# Set the Time column as the index for both DataFrames\n",
    "df2.set_index('Time', inplace=True)\n",
    "df3.set_index('Time', inplace=True)\n",
    "\n",
    "# Identify common columns between inference_df and the new DataFrames\n",
    "common_columns_df2 = df2.columns.intersection(inference_df.columns)\n",
    "common_columns_df3 = df3.columns.intersection(inference_df.columns)\n",
    "\n",
    "# Merge df2 and df3 into inference_df based on datetime index\n",
    "inference_df.update(df2[common_columns_df2])\n",
    "inference_df.update(df3[common_columns_df3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the cutoff timestamp\n",
    "cutoff_time = pd.Timestamp('2024-09-04 16:00')\n",
    "\n",
    "# Drop rows in inference_df that are after the cutoff timestamp\n",
    "inference_df = inference_df[inference_df.index <= cutoff_time]\n",
    "\n",
    "# Check the last few rows after cutoff\n",
    "inference_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define start and end dates for inference\n",
    "start_date = \"2024-08-30 00:00\"\n",
    "end_date = \"2024-09-07 23:00\"\n",
    "\n",
    "# Create an hourly date range for inference\n",
    "inference_index = pd.date_range(\n",
    "    start=pd.to_datetime(start_date),\n",
    "    end=pd.to_datetime(end_date),\n",
    "    freq=\"1h\"\n",
    ")\n",
    "\n",
    "# Define target variables and features for inference DataFrames\n",
    "target_vars_et = ['traffic_abs', 'sum_IN_abs', 'sum_OUT_abs', 'Lusen-Mauth-Finsterau IN', 'Lusen-Mauth-Finsterau OUT', \n",
    "               'Nationalparkzentrum Lusen IN', 'Rachel-Spiegelau IN', 'Rachel-Spiegelau OUT', 'Nationalparkzentrum Lusen OUT', 'Falkenstein-Schwellhäusl IN', 'Falkenstein-Schwellhäusl OUT', \n",
    "                        'Scheuereck-Schachten-Trinkwassertalsperre IN', 'Scheuereck-Schachten-Trinkwassertalsperre OUT', \n",
    "                        'Nationalparkzentrum Falkenstein IN', 'Nationalparkzentrum Falkenstein OUT']\n",
    "\n",
    "# Create an empty DataFrame for inference with required columns\n",
    "inference_dfs = {}\n",
    "\n",
    "for target in target_vars_et:\n",
    "    # Create an empty DataFrame for inference with necessary columns\n",
    "    inference_df = pd.DataFrame(index=inference_index, columns=numeric_features + categorical_features)\n",
    "\n",
    "    # Add date-related features using cyclic transformations (e.g., Hour, Monat)\n",
    "    inference_df[\"Hour\"] = inference_df.index.hour\n",
    "    inference_df[\"Monat\"] = inference_df.index.month\n",
    "    inference_df[\"Jahr\"] = inference_df.index.year\n",
    "\n",
    "    # Convert categorical features to string type\n",
    "    for catfeature in categorical_features:\n",
    "        inference_df[catfeature] = inference_df[catfeature].astype(str)\n",
    "    \n",
    "    # Ensure the DataFrame has a date-time index\n",
    "    inference_df.index.name = 'DateTime'\n",
    "    \n",
    "    # Load the pre-trained model for the target variable\n",
    "    model = load_model(f\"{save_path}/extra_trees_{target}\")\n",
    "\n",
    "    # Make predictions on the inference data\n",
    "    pred_unseen = predict_model(model, data=inference_df)\n",
    "    \n",
    "    # Process predictions (if needed), ensuring consistency with prior code\n",
    "    formatted_pred_unseen = pred_unseen  # Apply any necessary formatting or transformations here\n",
    "    \n",
    "    # Save predictions in dictionary\n",
    "    predictions_dict_et[f\"extra_trees_{target}\"] = formatted_pred_unseen\n",
    "\n",
    "    # Display the first few rows of predictions\n",
    "    print(f\"Predictions for {target}:\\n\", formatted_pred_unseen.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the unseen data using the pre-trained model\n",
    "#pred_unseen = predict_model(model, data=inference_df)\n",
    "\n",
    "# Process predictions (if needed), ensuring consistency with prior code\n",
    "#formatted_pred_unseen = pred_unseen  # Apply any necessary formatting or transformations here\n",
    "\n",
    "# Display the first few rows of predictions\n",
    "#formatted_pred_unseen.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inference_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pred_unseen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#scaler = MinMaxScaler()\n",
    "\n",
    "\n",
    "#formatted_pred_unseen = pred_unseen\n",
    "#formatted_pred_unseen.prediction_label = formatted_pred_unseen.prediction_label.round()\n",
    "#formatted_pred_unseen['weekly_relative_traffic'] = scaler.fit_transform(formatted_pred_unseen['prediction_label'].values.reshape(-1, 1))\n",
    "#formatted_pred_unseen.sort_values(by='weekly_relative_traffic',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_parquet_file_to_aws_s3(df: pd.DataFrame, path: str, **kwargs) -> pd.DataFrame:\n",
    "    \"\"\"Writes an individual Parquet file to AWS S3.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to write.\n",
    "        path (str): The path to the Parquet files on AWS S3.\n",
    "        **kwargs: Additional arguments to pass to the to_parquet function.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        wr.s3.to_parquet(df, path=path, **kwargs)\n",
    "        print(f\"DataFrame successfully written to {path}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to write DataFrame to S3. Error: {e}\")\n",
    "    return\n",
    "\n",
    "# Define the bucket and folder name as in your original code\n",
    "bucket = 'dssgx-munich-2024-bavarian-forest'\n",
    "preprocessed_data_folder = 's3://dssgx-munich-2024-bavarian-forest/preprocessed_data/'\n",
    "\n",
    "# Write each target variable's predictions to S3 as Parquet files\n",
    "for target in target_vars_et:\n",
    "    formatted_pred_unseen = predictions_dict_et[f\"extra_trees_{target}\"]\n",
    "    file_path = f\"s3://{bucket}/{preprocessed_data_folder}/predictions_{target}.parquet\"\n",
    "    \n",
    "    write_parquet_file_to_aws_s3(\n",
    "        df=formatted_pred_unseen,\n",
    "        path=file_path,\n",
    "        index=False  # Typically, you might not want to save the index in the Parquet file\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bavforest",
   "language": "python",
   "name": "bavforest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
